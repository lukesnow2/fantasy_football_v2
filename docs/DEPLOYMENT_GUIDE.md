# üöÄ Deployment Guide - Fantasy Football Data Pipeline

## üìã Overview

This guide provides complete setup and deployment procedures for the enterprise-grade fantasy football data pipeline with incremental processing, hybrid loading, and automated operations.

## üèóÔ∏è System Requirements

### Prerequisites
- **Python 3.9+** with pip
- **Yahoo Developer Account** with API credentials
- **PostgreSQL Database** (Heroku recommended)
- **GitHub Account** (for automation)

### Dependencies
```bash
pip install -r requirements.txt
```

## ‚ö° Quick Setup

### 1. Clone and Setup
```bash
git clone https://github.com/lukesnow-1/the-league.git
cd the-league
pip install -r requirements.txt
```

### 2. Configure Authentication
```bash
# Copy configuration template
cp data/templates/config.template.json config.json

# Edit config.json with your Yahoo API credentials:
# {
#   "consumer_key": "your_yahoo_client_id",
#   "consumer_secret": "your_yahoo_client_secret"
# }
```

### 3. Initial Authentication
```bash
# Run OAuth flow (creates oauth2.json automatically)
python3 src/auth/yahoo_oauth.py
```

### 4. Test Extraction
```bash
# Test incremental extraction (works year-round)
python3 scripts/weekly_extraction.py --force
```

### 5. Deploy Database
```bash
export DATABASE_URL="your-postgres-url"
python3 src/deployment/incremental_loader.py --data-file data/current/data.json
```

## üóÑÔ∏è Database Deployment

### Option 1: Hybrid Loading (Recommended)
Uses table-specific strategies for optimal performance:
```bash
# Deploy with incremental loading strategies
python3 src/deployment/incremental_loader.py --data-file data/current/data.json

# With specific database URL
python3 src/deployment/incremental_loader.py --data-file data/current/data.json --database-url $DATABASE_URL

# With verbose logging
python3 src/deployment/incremental_loader.py --data-file data/current/data.json --verbose
```

### Option 2: Legacy Deployment
Full table replacement (slower but simple):
```bash
python3 scripts/deploy.py --data-file data/current/data.json
```

### Option 3: EDW Deployment
Deploy with Enterprise Data Warehouse:
```bash
# Complete system (operational data + EDW)
python3 scripts/deploy_with_edw.py --data-file data/current/data.json

# EDW only
python3 scripts/deploy_with_edw.py --edw-only

# Operational data only
python3 scripts/deploy_with_edw.py --data-file data/current/data.json --operational-only
```

## ü§ñ GitHub Actions Setup

### Required Secrets
Configure these in GitHub Settings ‚Üí Secrets ‚Üí Actions:

```
YAHOO_CLIENT_ID=your_yahoo_client_id
YAHOO_CLIENT_SECRET=your_yahoo_client_secret
YAHOO_REFRESH_TOKEN=your_yahoo_refresh_token
HEROKU_DATABASE_URL=your_postgres_url
```

### Getting Yahoo Refresh Token
```bash
# Run authentication flow and note the refresh token
python3 src/auth/yahoo_oauth.py

# The refresh token will be in oauth2.json:
cat oauth2.json | grep refresh_token
```

### Workflow Configuration
The pipeline is pre-configured in `.github/workflows/weekly-data-extraction.yml`:

**Schedule**: Every Sunday 6 AM PST (Aug 18 - Jan 18)
**Manual Trigger**: Available via GitHub Actions interface

### Enable Notifications
1. Go to GitHub Profile ‚Üí Settings ‚Üí Notifications
2. Enable "Actions" email notifications
3. You'll receive alerts for success/failure

## üìä Data Validation & Monitoring

### Pre-Deployment Validation
```bash
# Check data files for duplicates before loading
python3 scripts/duplicate_detector.py --data-files data/current/*.json --alert-only

# Analyze data structure and get recommendations
python3 scripts/analyze_data_structure.py --data-file data/current/data.json
```

### Post-Deployment Validation
```bash
# Check live database for duplicates
python3 scripts/duplicate_detector.py --alert-only

# Detailed duplicate analysis with reporting
python3 scripts/duplicate_detector.py --output duplicate_report.json --detailed
```

### Real-Time Monitoring
```bash
# Monitor database integrity with alerting
python3 scripts/duplicate_detector.py --monitor --alert-threshold 1
```

## üîß Configuration Options

### Extraction Configuration
```bash
# Production run (respects season dates)
python3 scripts/weekly_extraction.py

# Force run during off-season
python3 scripts/weekly_extraction.py --force

# Historical extraction (if needed)
python3 scripts/full_extraction.py
```

### Loading Strategies
The hybrid loader uses table-specific strategies:

- **UPSERT**: `leagues`, `teams` (updates existing, inserts new)
- **INCREMENTAL_APPEND**: `rosters`, `matchups` (delete current week, insert fresh)
- **APPEND_ONLY**: `transactions`, `draft_picks` (skip existing, append new)

### Performance Tuning
```bash
# Check current performance metrics
python3 scripts/analyze_data_structure.py --data-file data/current/data.json --performance

# Compare extraction methods
python3 scripts/weekly_extraction.py --force --benchmark
```

## üèõÔ∏è Database Schema Management

### Core Schema (Operational)
```bash
# View schema from utils
cat src/utils/yahoo_fantasy_schema.sql

# Create tables manually if needed
psql $DATABASE_URL < src/utils/yahoo_fantasy_schema.sql
```

### EDW Schema (Analytics)
```bash
# Deploy complete EDW
python3 src/edw_schema/deploy_edw.py

# Force rebuild (drops existing)
python3 src/edw_schema/deploy_edw.py --drop-existing

# Test EDW deployment
python3 src/edw_schema/test_edw_deployment.py --verbose
```

### Schema Components
**Core Tables (6)**: leagues, teams, rosters, matchups, transactions, draft_picks
**Analytics Views (3)**: draft_analysis, team_draft_summary, player_draft_history
**EDW Tables (15)**: 6 dimensions + 5 facts + 4 marts

## üîê Security Configuration

### Credential Management
```bash
# Verify files are properly gitignored
git status  # Should NOT show oauth2.json or config.json

# Check for accidentally committed secrets
git log --grep="oauth\|secret\|token" --oneline

# Verify protection patterns
cat .gitignore | grep -E "(oauth|config|secret|token)"
```

### Production Security
- **GitHub Secrets**: Store credentials securely for CI/CD
- **Environment Variables**: Use for production deployments
- **OAuth Management**: Automatic token refresh and handling
- **Audit Trail**: Monitor access and changes

### Security Verification
```bash
# Check for sensitive files in git history
git log --all --full-history -- oauth2.json config.json
# Should return: (no output) ‚úÖ

# Verify current repository security
git ls-files | grep -E "(oauth2|config|secret|token)"
# Should only show safe template files
```

## üß™ Testing & Validation

### Component Testing
```bash
# Test core components
python3 -c "from src.extractors.weekly_extractor import IncrementalDataExtractor; print('‚úÖ')"
python3 -c "from src.deployment.incremental_loader import IncrementalLoader; print('‚úÖ')"
python3 -c "from src.auth.yahoo_oauth import YahooOAuth; print('‚úÖ')"
```

### Integration Testing
```bash
# Complete pipeline test
python3 scripts/weekly_extraction.py --force
python3 scripts/duplicate_detector.py --data-files data/current/*.json --alert-only
python3 src/deployment/incremental_loader.py --data-file data/current/data.json
python3 scripts/duplicate_detector.py --alert-only
```

### Performance Testing
```bash
# Benchmark extraction methods
time python3 scripts/weekly_extraction.py --force
time python3 scripts/full_extraction.py

# Benchmark loading methods
time python3 src/deployment/incremental_loader.py --data-file data/current/data.json
time python3 scripts/deploy.py --data-file data/current/data.json
```

## üõ†Ô∏è Troubleshooting

### Common Issues

#### Authentication Problems
```bash
# Check OAuth token validity
python3 -c "
import json
with open('oauth2.json') as f:
    tokens = json.load(f)
    print(f'Expires: {tokens.get(\"expires_at\", \"Unknown\")}')
"

# Refresh OAuth tokens
python3 src/auth/yahoo_oauth.py --refresh
```

#### Season Detection Issues
```bash
# Check season configuration
python3 -c "
from datetime import datetime
now = datetime.now()
print(f'Current date: {now}')
print(f'Season active: {8 <= now.month <= 12 or now.month == 1}')
"

# Force extraction during off-season
python3 scripts/weekly_extraction.py --force
```

#### Database Connection Issues
```bash
# Test database connection
python3 -c "
import os
import psycopg2
try:
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    print('‚úÖ Database connection successful')
    conn.close()
except Exception as e:
    print(f'‚ùå Database connection failed: {e}')
"
```

#### Data Integrity Issues
```bash
# Check for duplicates
python3 scripts/duplicate_detector.py --alert-only

# Analyze data structure
python3 scripts/analyze_data_structure.py --data-file data/current/data.json

# Validate schema compatibility
python3 -c "
from src.utils.database_schema import verify_schema
verify_schema()
print('‚úÖ Schema validation complete')
"
```

### Manual Recovery

#### Emergency Extraction
```bash
# Manual extraction if automation fails
python3 scripts/weekly_extraction.py --force

# Historical extraction for recovery
python3 scripts/full_extraction.py
```

#### Emergency Deployment
```bash
# Deploy with specific file
python3 scripts/deploy.py --data-file "path/to/backup/data.json"

# Force deployment with error override
python3 src/deployment/incremental_loader.py --data-file data/current/data.json --force
```

#### Database Recovery
```bash
# Backup current database
pg_dump $DATABASE_URL > backup_$(date +%Y%m%d).sql

# Restore from backup
psql $DATABASE_URL < backup_20241215.sql

# Reset schema and redeploy
python3 scripts/deploy.py --data-file data/current/data.json --reset-schema
```

## üìà Performance Optimization

### Extraction Optimization
- **Incremental processing**: 95% faster than full extraction
- **Baseline loading**: Smart reuse of historical data
- **Current season focus**: Avoid querying 20+ years of history
- **Parallel processing**: Concurrent API calls where possible

### Loading Optimization
- **Hybrid strategies**: Table-specific approaches for optimal performance
- **Batch operations**: Bulk inserts for improved throughput
- **Index optimization**: Strategic indexing for query performance
- **Connection pooling**: Efficient database resource usage

### Monitoring Performance
```bash
# Track extraction performance
python3 scripts/weekly_extraction.py --force --timing

# Track loading performance
python3 src/deployment/incremental_loader.py --data-file data/current/data.json --timing

# Database performance analysis
python3 -c "
from src.utils.database_loader import analyze_performance
analyze_performance()
"
```

## üìã Maintenance Schedule

### Daily (Automated)
- Season detection and eligibility checking
- Automated extraction during season (Sundays)
- Data integrity monitoring and alerting

### Weekly (During Season)
- Review automation logs and success/failure reports
- Monitor data growth and storage usage
- Verify email notifications are working

### Monthly
- Review extraction performance and optimization opportunities
- Check database performance and query optimization
- Validate backup and recovery procedures

### Annually
- Rotate Yahoo API credentials
- Review and update security configurations
- Performance benchmarking and system optimization
- Documentation updates and accuracy verification

---

**This deployment guide provides comprehensive coverage of setup, configuration, and operational procedures for the fantasy football data pipeline.** 